---
title: "COVID-19 in Newfoundland and Labrador"
subtitle: "Analysis and Modelling"
author: "John W"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
---

# Introduction

This [RStudio](https://global.rstudio.com/categories/rstudio-ide/) notebook studies a number of data sets published by **Health Canada** on COVID-19 cases, vaccinations and variants. They will be used in this case to study the progression of the disease in Newfoundland and Labrador.

# What are the most common strains of COVID-19 currently circulating within the province?

To answer this question, we can make use of a data set which tracks the proportion of variants of COVID-19 in the province. It is updated weekly as of January 18, 2025.

```{r}
library(tidyverse)
library(ggthemes)
library(caret)
library(randomForest)
library(glue)
library(RColorBrewer)

variants_df <- read.csv("https://health-infobase.canada.ca/src/data/covidLive/covid19-epiSummary-variants.csv")

# Clean the data and calculate days and proportions
variants_df <- variants_df |>
  janitor::clean_names() |>
  mutate(days = as.integer(as.Date(week_of_collection, format = "%Y-%m-%d") - min(as.Date(week_of_collection, format = "%Y-%m-%d")))) |>
  mutate(variant = as.factor(variant)) |>
  # Calculate proportions and median proportions for each variant
  group_by(variant) |>
  mutate(proportions = proportions / 100) |>
  mutate(median_proportion = median(proportions, na.rm = TRUE))

# Reorder 'variant' based on median_proportion in descending order
variant_order <- variants_df |>
  group_by(variant) |>
  summarise(median = median(median_proportion, na.rm = TRUE)) |>
  arrange(desc(median)) |>
  pull(variant)

variants_df$variant <- factor(variants_df$variant, levels = variant_order)

# Plotting the boxplots sorted by median proportions
ggplot(variants_df, aes(x = variant, y = proportions, group = variant, color = variant)) +
  geom_boxplot() + 
  labs(
    title = "Proportion of Different COVID-19 Variants Over Time",
    subtitle = glue::glue("Newfoundland and Labrador (Last Updated on {max(variants_df$week_of_collection)})"),
    x = "COVID-19 Variant",
    y = "Proportion of Variants",
    fill = "COVID-19 Variant"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "none",
    plot.title = element_text(hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5)
  ) +
  scale_color_viridis_d() +
  scale_y_continuous(labels = scales::percent)
  

```

There are at least 13 known strains of COVID-19 circulating in the province. As of January 18, 2025, the most common strain is the XEC strain, followed by the KP.3.1.1 and MC.1 strains, making them the dominant strains for the observed period. The XEC variant is perhaps the most prominent, though this picture may change as the data set is updated. In either case, the XEC strain (as of writing) has the highest median proportion of cases and a wide interquartile range, indicating some variability in its proportions over time.

# Which vaccines are most commonly administered in the province?

```{r}
vaccine_df <- read.csv("https://health-infobase.canada.ca/src/data/covidLive/vaccination-administration-bydosenumber_grouped.csv")

# Boxplot for vaccine groups ordered by maximum value with wrapped x labels
vaccine_df |>
  janitor::clean_names() |>
  filter(prename == "Newfoundland and Labrador") |>
  filter(vaccine_group != "Total" & vaccine_group != "Unknown") |>
  mutate(
    vaccine_group = factor(vaccine_group, 
                           levels = vaccine_df |>
                             filter(prename == "Newfoundland and Labrador") |>
                             group_by(vaccine_group) |>
                             summarise(median_doses = median(numtotal_totaldoses_admin, na.rm = TRUE)) |>
                             arrange(desc(median_doses)) |>
                             pull(vaccine_group)),
    wrapped_vaccine_group = str_wrap(vaccine_group, width = 20)  # Wrap the text for x labels
  ) |>
  ggplot(aes(x = wrapped_vaccine_group, y = numtotal_totaldoses_admin, color = vaccine_group)) +
  geom_boxplot() +
  labs(
    title = str_wrap("Total Doses Administered by Vaccine Group", width = 50),
    subtitle = glue::glue("Newfoundland and Labrador (Last Updated on {max(vaccine_df$week_end)})"),
    x = "Vaccine Group",
    y = "Total Doses Administered"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),  # Adjusting angle for right-side up labels
    legend.position = "none",
    plot.title = element_text(hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5),
  ) +
  scale_y_continuous(labels = scales::number) +
  scale_color_viridis_d()






```

The Pfizer-BioNTech Comirnaty vaccines (with the original strain) have been administered in the province in the largest numbers.

# Random Forest Model

To study the progression of COVID-19 in the province, we will make use of the **random forest model.** The random forest model is an "ensemble machine learning algorithm that combines multiple decision trees to improve the accuracy, robustness, and generalization of predictions. It is widely used for both regression and classification tasks and is particularly effective for handling large, complex datasets with a variety of input features."<sup>1</sup>

## Plots

```{r}
# Load and prepare the data
data <- read.csv("https://health-infobase.canada.ca/src/data/covidLive/covid19-download.csv")

df <- data |>
  filter(prname == "Newfoundland and Labrador") |>
  mutate(
    totalcases = as.numeric(totalcases),
    days = as.integer(as.Date(date, format = "%Y-%m-%d") - min(as.Date(date, format = "%Y-%m-%d"))),  # Calculate days since the first case
    sqrt_totalcases = sqrt(totalcases)  # Square root transformation of the totalcases
  ) |>
  drop_na(sqrt_totalcases) |>
  filter(!is.na(totalcases) & !is.infinite(totalcases)) |>
  filter(totalcases > 0)

# Prepare feature matrix (X) and target variable (y)
X <- df |>
  select(days) |>
  as.matrix()

y <- df$sqrt_totalcases  # Target variable

# Convert X and y to numeric matrix and vector, respectively
X <- as.matrix(X)  # Ensure X is a matrix
y <- as.numeric(y)  # Ensure y is a numeric vector

# Split the data into training and test sets
set.seed(123)
train_index <- createDataPartition(y, p = 0.8, list = FALSE)
X_train <- X[train_index, , drop = FALSE]  # Ensure X_train is a matrix
y_train <- y[train_index]
X_test <- X[-train_index, , drop = FALSE]  # Ensure X_test is a matrix
y_test <- y[-train_index]

# Convert to data frame for randomForest training
train_df <- data.frame(days = X_train, sqrt_totalcases = y_train)
test_df <- data.frame(days = X_test, sqrt_totalcases = y_test)

# Train the Random Forest model using caret
rf_model <- randomForest(sqrt_totalcases ~ days, data = train_df, ntree = 3, mtry = 1, importance = TRUE)

# Print model summary
print(rf_model)

# Make predictions on the test set
y_pred <- predict(rf_model, newdata = test_df)

# Evaluate the model on the test set
rmse <- sqrt(mean((y_test - y_pred)^2))
mae <- mean(abs(y_test - y_pred))
r2 <- 1 - sum((y_test - y_pred)^2) / sum((y_test - mean(y_test))^2)

# Cross-validation with caret
cv_model <- train(
  sqrt_totalcases ~ days, 
  data = train_df, 
  method = "rf", 
  trControl = trainControl(method = "cv", number = 5),  # 5-fold cross-validation
  tuneGrid = expand.grid(mtry = 1),  # Adjust this grid for hyperparameter tuning
  ntree = 3
)

# Cross-validation evaluation metrics
cv_rmse <- min(cv_model$resample$RMSE)
cv_mae <- min(cv_model$resample$MAE)
cv_r2 <- max(cv_model$resample$Rsquared)

# Create the subtitle with performance metrics
subtitle_text <- glue(
  "Test RMSE: {round(rmse, 2)} | Test MAE: {round(mae, 2)} | Test R²: {round(r2, 2)} | ",
  "CV RMSE: {round(cv_rmse, 2)} | CV MAE: {round(cv_mae, 2)} | CV R²: {round(cv_r2, 2)}"
)

# Wrap the subtitle text to fit within the plot using str_wrap
wrapped_subtitle <- str_wrap(subtitle_text, width = 80)

# Fitted curve for the full data
full_data <- data.frame(days = df$days, sqrt_totalcases = df$sqrt_totalcases)
full_data$predictions <- predict(rf_model, newdata = full_data)

# Extend the range of days for prediction (next 365 days)
future_days <- data.frame(days = (max(df$days) + 1):(max(df$days) + 365))
future_predictions <- predict(rf_model, newdata = future_days)

# Plot 2: Learning Curves (Training and Test RMSE)
# Evaluate learning curve over different ntree values
ntree_values <- seq(1, 3, by = 1)
train_rmse <- sapply(ntree_values, function(ntree) {
  rf_model <- randomForest(sqrt_totalcases ~ days, data = train_df, ntree = ntree, mtry = 1)
  sqrt(mean((y_train - predict(rf_model, newdata = train_df))^2))
})
test_rmse <- sapply(ntree_values, function(ntree) {
  rf_model <- randomForest(sqrt_totalcases ~ days, data = train_df, ntree = ntree, mtry = 1)
  sqrt(mean((y_test - predict(rf_model, newdata = test_df))^2))
})

# Create learning curve data frame
learning_curve_data <- data.frame(
  ntree = rep(ntree_values, 2),
  rmse = c(train_rmse, test_rmse),
  dataset = rep(c("Training", "Test"), each = length(ntree_values))
)

# Generate synthetic data for fitting the curve (logistic growth with noise)
set.seed(123)
days_synthetic <- 1:100
totalcases_synthetic <- 100 * (1 / (1 + exp(-0.1 * (days_synthetic - 50)))) + rnorm(100, 0, 10)  # Logistic growth with noise
df_synthetic <- data.frame(days = days_synthetic, totalcases = totalcases_synthetic)

# Prepare synthetic data feature matrix and target variable
X_synthetic <- df_synthetic |>
  select(days) |>
  as.matrix()

y_synthetic <- df_synthetic$totalcases  # Target variable for synthetic data

# Train Random Forest on synthetic data
train_df_synthetic <- data.frame(days = X_synthetic, totalcases = y_synthetic)

# Train the Random Forest model for synthetic data
rf_model_synthetic <- randomForest(totalcases ~ days, data = train_df_synthetic, ntree = 3, mtry = 1)

# Make predictions for the synthetic data
synthetic_pred <- predict(rf_model_synthetic, newdata = train_df_synthetic)

# Evaluate the model on synthetic data
rmse_synthetic <- sqrt(mean((y_synthetic - synthetic_pred)^2))
mae_synthetic <- mean(abs(y_synthetic - synthetic_pred))
r2_synthetic <- 1 - sum((y_synthetic - synthetic_pred)^2) / sum((y_synthetic - mean(y_synthetic))^2)  # Calculate R²

# Cross-validation for synthetic data
cv_model_synthetic <- train(
  totalcases ~ days, 
  data = train_df_synthetic, 
  method = "rf", 
  trControl = trainControl(method = "cv", number = 5),  # 5-fold cross-validation
  tuneGrid = expand.grid(mtry = 1),  # Adjust this grid for hyperparameter tuning
  ntree = 3
)

# Cross-validation evaluation metrics for synthetic data
cv_rmse_synthetic <- min(cv_model_synthetic$resample$RMSE)
cv_mae_synthetic <- min(cv_model_synthetic$resample$MAE)
cv_r2_synthetic <- max(cv_model_synthetic$resample$Rsquared)

# Create the subtitle for synthetic data with performance metrics
subtitle_text_synthetic <- glue(
  "Test RMSE: {round(rmse_synthetic, 2)} | Test MAE: {round(mae_synthetic, 2)} | Test R²: {round(r2_synthetic, 2)} | ",
  "CV RMSE: {round(cv_rmse_synthetic, 2)} | CV MAE: {round(cv_mae_synthetic, 2)} | CV R²: {round(cv_r2_synthetic, 2)}"
)

# Wrap the subtitle text for synthetic data
wrapped_subtitle_synthetic <- str_wrap(subtitle_text_synthetic, width = 80)


```

### Fitted Curve Plot

```{r}
  # Fitted curve for full data, with performance metrics in the subtitle.
ggplot() +
  geom_point(data = full_data, aes(x = days, y = sqrt_totalcases), color = brewer.pal(3,"Dark2")[1], size=3) +  # Actual data points
  geom_line(data = full_data, aes(x = days, y = predictions), color = brewer.pal(3,"Dark2")[2], linewidth=1) +  # Fitted curve
  geom_line(data = data.frame(days = future_days$days, predictions = future_predictions), aes(x = days, y = predictions),linetype="dashed", color = brewer.pal(3,"Dark2")[3], linewidth=1) +  # Predictions for next 365 days as blue dashed line
  labs(
    title = str_wrap("COVID-19 in Newfoundland and Labrador Fitted and Predicted (Dashed Line Shows Predictions for Next 365 Days)",width=50),
    x = "Days", y = "Sqrt(Total Cases)",
    subtitle = wrapped_subtitle
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust=0.5),
    plot.subtitle = element_text(hjust = 0.5)  # Center the subtitle
  )
```

The first plot in the series shows a curve, predicted by the model, fitted to the full data set. The RMSE is low, indicating that the model's predictions are close to that of the observed values. The MAE is also low, suggesting a good model fit with very small errors. The $R^2$ value is close to one, indicating a nearly ideal fit.

With respect to cross-validation metrics, the CV RMSE is slightly worse than that of the test RMSE, suggesting that the model performs slightly worse on different subsets of the data. Apart from that, it is quite reasonably low in value. The CV MAE is also low, indicating that the model performed well across folds, but was slightly less accurate on the test data. The CV $R^2$ is high; the model explains nearly all of the variability in the test data.

### Learning Curves

```{r}
  # Learning curves.
ggplot(learning_curve_data, aes(x = ntree, y = rmse, color = dataset)) +
  geom_line(linewidth=1) +
  geom_point(size=3) +
  labs(
    title = "Learning Curves for Random Forest",
    x = "Number of Trees (ntree)",
    y = "Root Mean Squared Error (RMSE)",
    subtitle = "Training vs. Test Performance for Different Numbers of Trees",
    color="Legend"
  ) +
  theme_minimal() +
  scale_color_manual(values = c(brewer.pal(3,"Dark2")[1], brewer.pal(3,"Dark2")[2])) +
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.subtitle = element_text(hjust=0.5),
  )
```

The learning curves plot "is a graphical representation that shows how a machine learning model's performance improves as it is trained on more data or with different model configurations (such as the number of trees in a [random forest], or the number of training iterations in other models)."<sup>1</sup> It is useful in diagnosing issues with a model such as overfitting and underfitting, as well as for evaluating the model's ability to generalize.

The learning curves plot in this case indicates that at only 1 tree, the model is underfitting the data. As the number of trees are increased to 2, there is a significant decrease in both test and training RMSE. This is consistent with better generalization and less underfitting. At 3 trees, both test and training RMSE stabilize at roughly the same level. This suggests that increasing the number of trees beyond 3 does not significantly improve the model's performance.

### Synthetic Data

```{r}
  # Plot: Fitted Curve for Synthetic Data with Performance Metrics in the Subtitle
ggplot() +
  geom_point(data = df_synthetic, aes(x = days, y = totalcases), color = brewer.pal(3,"Dark2")[1],size=3) +  # Actual synthetic data points
  geom_line(data = df_synthetic, aes(x = days, y = synthetic_pred), color = brewer.pal(3,"Dark2")[2], linewidth = 1) +  # Fitted curve for synthetic data
  labs(
    title = "Fitted Curve for Synthetic Data",
    x = "Days", y = "Total Cases",
    subtitle = wrapped_subtitle_synthetic
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust=0.5),
    plot.subtitle = element_text(hjust = 0.5)  # Center the subtitle
  )
```

How does the model perform against new and unseen data? The synthetic data plot and fitted curve simulates this condition. It is obvious from the performance metrics that despite the synthetic data being entirely new to the model, it performs reasonably well. The test $R^2$ is high, indicating a good fit to the synthetic data. The test RMSE is conversely low; this is an indication that the model's prediction errors are quite reasonable when the scale of total case numbers is taken into account. The test MAE is similarly low; the model's predictions are off by a very small amount on average.

With respect to cross-validation metrics, the CV $R^2$ is also high, though slightly lower than the test $R^2.$ This suggests that while the model still performs quite well, there is a small amount of variability in its generalization to different data splits. The CV RMSE is low, though slightly higher than the test RMSE; the model experiences some fluctuation in performance across different folds of the data, but it is still a strong result. Finally, the CV MAE is low, though again slightly higher than the test MAE, suggesting some variability in the model's performance.

# Conclusion

We have provided a general picture of the number of cases of different variants of COVID-19 in Newfoundland and Labrador, Canada. We have also examined the number and commercial name of vaccines distributed in the province. Finally, we have evaluated the effectiveness of the random forest model in its ability to describe and predict the progression of the disease in the province. The random forest model provides an excellent basis on which this progression can be described and predicted. Its strengths in modelling outbreaks of the disease may be summarized as follows:

- COVID-19 data often exhibits complex, non-linear relationships between variables. The random forest model is able to handle this complexity through its use of a collection of decision trees.

- COVID-19 data is often large and complex in its own right, particularly when factors like geography, testing rates and vaccination are taken into consideration. The random forest model is particularly well suited to such data sets, as it "can handle high-dimensional data with many predictors and complex interactions."<sup>1</sup>

- Random forest makes use of multiple decision trees using different subsets of the data, averaging their predictions. This helps to generalize the model and avoid the problem of overfitting.

- Random forest has a built-in method for evaluating feature importance. This is critical in the analysis of COVID-19, as it allows for one to identify which variables (e.g. government interventions, vaccinations) are most important in predicting the outcomes (in this case, total case count).

- COVID-19 data is often less than ideal in form, containing missing values due to incomplete reporting or other factors. Random forest is, by nature, less sensitive to this issue. It makes use of "surrogate splits in decision trees when data for a particular feature is unavailable."<sup>1</sup>

- Random forest is capable of handling both continuous and categorical variables. This is useful as COVID-19 data can often contain a variety of features, both categorical and continuous. Random forest can handle both of these types without any additional pre-processing.

- Random forest is particularly good at making predictions based on historical data. It can make future predictions based on such data, critical for intervention and planning strategies. In addition, it is capable of capturing the sudden shift in trends that can often occur in an outbreak of the disease (e.g. the arrival of a new variant in the province).

- Irregularities or sudden spikes in cases can interfere with the predictions of a single model. The "ensemble" approach of random forest means that noise and outliers are effectively mitigated, important when dealing with COVID-19 data.

- Random forest is highly scalable; it can handle large data sets efficiently.

- Many factors interact with each other in a COVID-19 outbreak, such as lockdown measures, vaccination rates and population density. Random forest is good at detecting complex interactions between features; in addition, these features do not need to be explicitly specified in the model.

In short, the random forest model is flexible, robust and scalable, three critical features which make it an excellent choice in modelling and predicting the progression of a COVID-19 outbreak. In reality, no one statistical model is used in describing and predicting an outbreak of the disease; there are too many complex dynamics and factors to be considered. Despite this, the random forest model has proven itself adept at describing and predicting the progression of COVID-19 in Newfoundland and Labrador.

# References

<sup>1</sup> - OpenAI. (2025). ChatGPT (January 17 version) [Large language model]. https://chat.openai.com/chat